---
id: 1865
title: 'Подождать - лучшая стратегия?'
date: '2020-01-07T15:43:43+00:00'
author: serge
layout: post
guid: 'https://sotnyk.ga/?p=1865'
permalink: /2020/01/07/podozhdat-luchshaya-strategiya/
---

Прошедший год прошел под знаком превосходства трансформеров на различных NLP задачах. И многое для того, чтобы с ними было удобно работать, сделала компания [Hugging Face](https://huggingface.co/).

В самом начале следующего года, нам обещают повышение производительности токенайзеров до 80 раз, легкость в генерации текста:

[![huggingface transformers library fast tokenizer and sequence generation](https://sotnyk.github.io/wp-content/uploads/2029/02/tokenizers.jpg)](https://www.linkedin.com/posts/thomas-wolf-a056857_nlp-deeplearning-opensource-activity-6616384232442986497-gc9l)

Также, трансформеры типа BERT хорошо известны своей прожорливостью в плане ресурсов. Одним из решений данной проблемы может стать [архитектура “Reformer” от Гугла](https://openreview.net/forum?id=rkgNKkHtvB). И то, что [Hugging Face заинтересовались этим](https://www.linkedin.com/posts/thomas-wolf-a056857_nlp-deeplearning-ai-activity-6619363933658714112-MQ0K), расслабляет ))) А может ничего не делать, и все будет сделано за нас?

Ну и напоследок – это один интересный обзор методов генерации текста от главного по науке в Hugging Face – [What happened in Natural language generation decoders in 2019?](https://www.linkedin.com/pulse/what-happened-natural-language-generation-decoders-20182019-wolf/)