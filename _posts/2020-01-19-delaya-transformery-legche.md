---
id: 1867
title: 'Делая трансформеры легче'
date: '2020-01-19T21:18:18+00:00'
author: serge
layout: post
guid: 'https://sotnyk.ga/?p=1867'
permalink: /2020/01/19/delaya-transformery-legche/
---

Для многих задач, трансформеры (нейросети, построенные в основном на основе механизма внимания – BERT, GPT, etc.), все еще являются слишком тяжелыми. Типичные модели занимают гигабайты и десятки гигабайт памяти, а скорость вывода не на GPU может удручать. Именно поэтому, очень важны исследования в области “облегчения” трансформеров – именно это может привести их во многие проекты, которые до этого отказывались от них.

В прошлом посте я упомянул архитектурную модификацию от [Гугла, названную Reformer](https://openreview.net/forum?id=rkgNKkHtvB).

Сегодня оставлю еще ссылки на информацию по теме (прежде всего для себя), за чем следить:

[Speeding up BERT](https://blog.inten.to/speeding-up-bert-5528e18bb4ea) – статья Григория Сапунова с обзором способов ускорения/уменьшения размера как старых трансформеров, так и архитектурные изменения, позволяющие уменьшить количество связей/нейронов в самой архитектуре трансформера. Особенно интересно было читать про ALBERT – именно эта архитектура показалась перспективной. К сожалению, пока не вижу для них многоязычных моделей, а многоязычность важна для моей текущей работы. Но буду внимательно следить – весьма вероятно появления в обозримом будущем и многоязычных моделей для этой многообещающей (конечно, если раньше не подвезут Реформера) архитектуры.

[Tiny-BERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT) – тут уже постарался Хуавей. Из описания: TinyBERT в 7.5x раз меньше и 9.4x быстрее при работе обученной сети, чем базовый BERT-base, при этом ожидается сравнимое качество на задачах понимания естественного языка (NLU). Фишка данного подхода в двойной дистилляции – вначале обычной, на универсальной модели, а затем уже на датасете, специфическом для задачи.

[Imagining a world without Transformers](https://towardsdatascience.com/imagining-a-world-without-transformers-single-headed-attention-rnn-844cca2580f9) — Single Headed Attention RNN. В этой статье напоминание, что трансформеры – это классно, но все же многие задачи можно решить не только с ними. В качестве примера выбрана рекуррентная нейросеть с одной головой внимания. Я бы в таких случаях предложил еще попробовать [CNN с вниманием](https://github.com/serge-sotnyk/seq2seq-compress) – с ней работать намного быстрее, особенно на GPU.