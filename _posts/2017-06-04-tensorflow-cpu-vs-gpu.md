---
id: 1707
title: 'Tensorflow. CPU vs. GPU'
date: '2017-06-04T23:18:26+00:00'
author: serge
layout: post
guid: 'http://sotnyk.ml/?p=1707'
permalink: /2017/06/04/tensorflow-cpu-vs-gpu/
---

[![](http://localhost/wp-content/uploads/2017/06/TURBO-GTX1080-8G_2D_500-300x180.png)](http://localhost/wp-content/uploads/2017/06/TURBO-GTX1080-8G_2D_500.png)

Наконец взял себе видеокарту, которая позволяет аппаратно ускорять [Tensorflow](https://www.tensorflow.org/). На удивление, все установилось под Windows (как сам Tensorflow, так и поддержка Cuda, CuDNN) без особых плясок с бубном, просто по инструкциям с сайта Tensorflow и NVIDIA. Единственное, что нужно внимательно следить за тем, какую версию библиотек и питона ставить – если сказано, что нужна определенная, то не нужно устанавливать самую последнюю. Это единственное, что не позволило запуститься совсем уж быстро.

Интересно стало, что это дает по сравнению с чисто CPU. Итак, небольшой Benchmark на коленке.

Не пишу ничего своего, просто решил запустить на примере, идущем в поставке Tensorflow – tensorflowexamplestutorialsword2vec

Для того, чтобы включить или выключить использование GPU, не изменяя программу, используем переменную окружения CUDA\_VISIBLE\_DEVICES.

Для отключения даем в консоли такую команду:  
set CUDA\_VISIBLE\_DEVICES=””

Опять включить можем просто удалив данную переменную:  
set CUDA\_VISIBLE\_DEVICES=

Чтобы посчитать суммарное время выполнения примера, я использовал небольшую [утилитку ptime](http://www.pc-tools.net/win32/ptime/). Загрузим её и дальше просто добавляем в начало команды ptime и она в конце вывода выводит нам суммарное время.

Итак, меряем. Запускаем Anaconda prompt. У меня исходники этого примера из тензорфлоу находятся в d:gittensorflowtensorflowexamplestutorialsword2vec, вы же у себя используйте свои пути. Первый запуск – просто для того, чтобы загрузились файлы с данными – при последующих запусках (с ptime) они уже не загружаются. Лишний вывод опущу.

\[code\]  
&gt; d:  
&gt; cd d:gittensorflowtensorflowexamplestutorialsword2vec  
&gt; python word2vec\_basic.py  
…  
&gt; ptime python word2vec\_basic.py  
…  
Execution time: 321.722 s  
&gt; set CUDA\_VISIBLE\_DEVICES=””  
&gt; ptime python word2vec\_basic.py  
…  
Execution time: 186.025 s  
\[/code\]

Поясню результат – первый раз мы запустили на GPU и время выполнения оказалось почти вдвое больше чем на чистом CPU! Печально конечно, но объяснимо – если модель нейросети большая и идет постоянная загрузка-выгрузка её, то на этом можно достаточно много потерять. Конечно, нужно будет подробнее посмотреть, о чем пример – наверняка, там не обучение самой модели word2vec (для этого самим гуглом использовался немаленький кластер), а какое-то использование уже готовой модели, но сейчас скорее важно, что в некоторых сценариях использование GPU не ускоряет работу, а наоборот.

Но все же – что будет, если самостоятельно учить сетку? Попробуем на сей раз заезженный вдоль и поперек MNIST (разогревочный запуск я опущу):

\[code\]  
&gt; cd D:gittensorflowtensorflowexamplestutorialsmnist  
&gt; set CUDA\_VISIBLE\_DEVICES=  
&gt; ptime python mnist\_deep.py  
…  
Execution time: 104.393 s  
&gt; set CUDA\_VISIBLE\_DEVICES=””  
&gt; ptime python mnist\_deep.py  
…  
Execution time: 2663.202 s  
\[/code\]

Фух, ну наконец-то! На классическом примере, GPU дала скорость в 25 раз больше, чем CPU. Все-же она того стоила…

Конфигурация, на которой я запускал примеры:

CPU: i5-4440@3.1GHz, RAM: 16Gb  
Video: GeForce GTX 1080, DDR5-8Gb