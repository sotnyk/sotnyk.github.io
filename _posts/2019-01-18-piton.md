---
id: 1815
title: Питон
date: '2019-01-18T19:35:12+00:00'
author: serge
layout: post
guid: 'https://sotnyk.ga/?p=1815'
permalink: /2019/01/18/piton/
---

<div class="wp-block-image"><figure class="alignleft">![](http://localhost/wp-content/uploads/2019/01/plan-reality-300x268.jpg)</figure></div>Последний месяц плотно подсел на питон. Все же устал бороться с тем, что в .Net по сравнению с Питоном почти пустынно в плане библиотек и фреймворков для ML, NLP.

Знания языка стали более твердыми, гораздо реже требуется залезать в поиск. При этом все еще часто бывают дурацкие ошибки, связанные с опечатками. Это не уйдет до конца. Именно поэтому и ненавижу скриптовые языки, в которых все вылавливаешь в рантайме. Но приходится с этим жить.

Как жить в Питоне:

- Писать юнит-тесты и почаще прогонять их. Кстати, pytest очень неплох.
- Хорошая IDE с постоянным анализом кода дополнительно уменьшает вероятность опечаток и других ошибок, связанных с человеческим фактором. Пока самая лучшая IDE – это PyCharm. Попробовал её месяц и купил. Она стоит своих денег. Производительность написания кода и поиска ошибок поднялась по сравнению с тем же VS Code (который использовал до этого).

Что хорошего в этом языке – он честный. Честный в том плане, что есть доступ ко многим внутренним механизмам, которые можно в случае необходимости подменить, подправить. И они достаточно несложные.

На C# я уже немного отвык ковыряться в исходном коде чужих фреймворков. В Питоне это получается делать постоянно. Этому способствует и краткость самого языка – за счет этого исходники обычно не такие уж и длинные.

В NLP сейчас царство эмбедингов. В вектора переводятся слова, предложения, целые тексты… Поначалу, когда впервые встретил [Word2Vec ](https://habr.com/ru/post/249215/)на хабре, все было понятно, просто и логично. Сейчас всплывают новые и новые тонкости и понимания гораздо меньше. Это обычное дело – чем больше знаешь, тем больше понимаешь сколько ты еще не знаешь.

На Питон перешел, поскольку тут потенциально есть все. Но вот в NLP оказалось, что есть один важный нюанс – все есть для английского, поэтому для других языков (особенность моих проектов) приходится по крупицам собирать многоязычные библиотеки, модели, словари.

Из открытий, которые вошли или очень похоже войдут в повседневное использование:

- [Spacy](https://spacy.io/) – прекрасный фреймворк для разбора текстов. Многие функции работают на основных европейских языках. Пайплайн очень кастомизируем (хотя, конечно, основная ценность в том, что многое можно использовать прямо “из коробки”). И просто очень много всего полезного. Вот сегодня хотел реализовать один из алгоритмов выделения noun chunk. В конце концов просто использовал в spacy свойство документа doc.noun\_chunks. Как “первый подскок” подошло очень даже неплохо, а там видно будет. Жаль, что нет нормальной поддержки украинского/русского, а в том же немецком нет встроенной векторизации строк.
- [Byte-Pair Encoding (BPE)](https://nlp.h-its.org/bpemb) – библиотека для векторизации слов с очень компактным представлением модели для 275 языков! За счет того, что используется subwords информация, несмотря на малые размеры модели, удается работать даже с незнакомыми словами (содержащими при этом фрагменты других известных слов) или словами с опечатками. И получается, что на многих задачах 11 мегабайтная модель BPEmb справляется не хуже, чем 6 гигабайтная (и это только для английского) FastText. Спасибо Юле Макогон, что дала ссылку на это чудо!

Еще, для тех, кто переключается на Питон из других языков, очень полезной будет книга [Рамальо Лучано – Python. К вершинам мастерства](https://www.litres.ru/luchano-ramalo/python-k-vershinam-masterstva-22805846/) (в оригинале Fluent Python). По мере её прочтения действительно проникаешься и стараешься писать в правильном Pythonic way (типичное выражение в вопросах на stack overflow). В общем, рекомендую эту книгу, а сам пошел дальше векторизовать тексты…